{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of LFW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DtTjcmduu0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "29b2b25d-2ba2-48c6-b68b-3c28cd53ceb8"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import random\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "\n",
        "class ConvAutoencoder:\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, filters=(32, 64), latentDim=16):\n",
        "\t\t# initialize the input shape to be \"channels last\" along with\n",
        "\t\t# the channels dimension itself\n",
        "\t\t# channels dimension itself\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "\t\t# define the input to the encoder\n",
        "\t\tinputs = Input(shape=inputShape)\n",
        "\t\tx = inputs\n",
        "\n",
        "\t\t# loop over the number of filters\n",
        "\t\tfor f in filters:\n",
        "\t\t\t# apply a CONV => RELU => BN operation\n",
        "\t\t\tx = Conv2D(f, (3,3), strides=1, padding=\"same\")(x)\n",
        "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\n",
        "\t\t# flatten the network and then construct our latent vector\n",
        "\t\tvolumeSize = K.int_shape(x)\n",
        "\t\tx = Flatten()(x)\n",
        "\t\tlatent = Dense(latentDim)(x)\n",
        "\n",
        "\t\t# build the encoder model\n",
        "\t\tencoder = Model(inputs, latent, name=\"encoder\")\n",
        "\n",
        "\n",
        "\t\t# start building the decoder model which will accept the\n",
        "\t\t# output of the encoder as its inputs\n",
        "\t\tlatentInputs = Input(shape=(latentDim,))\n",
        "\t\tx = Dense(np.prod(volumeSize[1:]))(latentInputs)\n",
        "\t\tprint(volumeSize)\n",
        "\t\tx = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
        "\n",
        "\t\t# loop over our number of filters again, but this time in\n",
        "\t\t# reverse order\n",
        "\t\tfor f in filters[::-1]:\n",
        "\t\t\tprint(f)\n",
        "\t\t\t# apply a CONV_TRANSPOSE => RELU => BN operation\n",
        "\t\t\tx = Conv2DTranspose(f, (3,3), strides=1,\n",
        "\t\t\t\tpadding=\"same\")(x)\n",
        "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\n",
        "\t\t# apply a single CONV_TRANSPOSE layer used to recover the\n",
        "\t\t# original depth of the image\n",
        "\t\tx = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
        "\t\toutputs = Activation(\"sigmoid\")(x)\n",
        "\n",
        "\t\t# build the decoder model\n",
        "\t\tdecoder = Model(latentInputs, outputs, name=\"decoder\")\n",
        "\n",
        "\t\t# our autoencoder is the encoder + decoder\n",
        "\t\tautoencoder = Model(inputs, decoder(encoder(inputs)),\n",
        "\t\t\tname=\"autoencoder\")\n",
        "\n",
        "\t\tencoder.summary()\n",
        "\t\tprint(\"=====================\")\n",
        "\t\tdecoder.summary()\n",
        "\t\tprint(\"==========================\")\n",
        "\t\tautoencoder.summary()\n",
        "\t\t# return a 3-tuple of the encoder, decoder, and autoencoder\n",
        "\t\treturn (encoder, decoder, autoencoder)\n",
        "  \n",
        "# USAGE\n",
        "# python train_unsupervised_autoencoder.py --dataset output/images.pickle --model output/autoencoder.model\n",
        "\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "\n",
        "\n",
        "def batch_generator(Train_dt,batch_size,steps):\n",
        "\tidx = 1\n",
        "\twhile True:\n",
        "\t\tyield getImages(batch_size, Train_dt, idx-1)\n",
        "\t\tif idx<steps:\n",
        "\t\t\tidx += 1\n",
        "\t\telse:\n",
        "\t\t\tidx = 1\n",
        "    \t\n",
        "    \t\n",
        "    \t\n",
        "def getImages(batchSize, folderName,idx):\n",
        "\tnames = os.listdir(folderName)\n",
        "\tskiprows = int(idx*batchSize)\n",
        "\tbatchout = []\n",
        "\tstart = skiprows\n",
        "\tend = skiprows +batchSize\n",
        "\twhile start<end :\n",
        "\t\tbatchout.append(names[start])\n",
        "\t\tstart += 1\n",
        "\timages = []\n",
        "\tfor i in batchout:\n",
        "\t\timages.append(np.array(Image.open(os.path.join(folderName,i))))\n",
        "\treturn (np.array(images), np.array(images))\n",
        " \n",
        "\"\"\"\n",
        "# construct the argument parse and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-m\", \"--model\", type=str,default=\"lfwautoencoder_latest.model\",\n",
        "\thelp=\"path to output trained autoencoder\")\n",
        "ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot_latest.png\",\n",
        "\thelp=\"path to output plot file\")\n",
        "args = vars(ap.parse_args())\n",
        "\"\"\"\n",
        "\n",
        "# initialize the number of epochs to train for, initial learning rate,\n",
        "# and batch size\n",
        "\n",
        "# construct our convolutional autoencoder\n",
        "INIT_LR = 1e-6\n",
        "print(\"[INFO] building autoencoder...\")\n",
        "BS = 32\n",
        "nb_epochs = 10\n",
        "train_data = os.listdir(\"/content/drive/My Drive/data/train\")\n",
        "steps_per_epoch_train = (len(train_data)//BS)\n",
        "\n",
        "my_training_batch_generator = batch_generator(\"/content/drive/My Drive/data/train\", BS,steps_per_epoch_train)\n",
        "val_data = os.listdir(\"/content/drive/My Drive/data/val\")\n",
        "steps_per_epoch_val = len(val_data)//BS\n",
        "my_val_batch_generator = batch_generator(\"/content/drive/My Drive/data/val\",BS,steps_per_epoch_val)\n",
        "\n",
        "(encoder, decoder, autoencoder) = ConvAutoencoder.build(250, 250, 3)\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / nb_epochs)\n",
        "autoencoder.compile(loss=\"mse\", optimizer=opt)\n",
        "\n",
        "# train the convolutional autoencoder\n",
        "H = autoencoder.fit_generator(\n",
        "\tmy_training_batch_generator,\n",
        "\tepochs=nb_epochs,steps_per_epoch = steps_per_epoch_train,verbose=1,\n",
        "\tvalidation_data= my_val_batch_generator,validation_steps = steps_per_epoch_val)\n",
        "# use the convolutional autoencoder to make predictions on the\n",
        "# testing images, construct the visualization, and then save it\n",
        "# to disk\n",
        "\n",
        "\n",
        "# construct a plot that plots and saves the training history\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig('my_figure.png')\n",
        "\n",
        "# serialize the image data to disk\n",
        "print(\"[INFO] saving image data...\")\n",
        "\n",
        "autoencoder.save_weights('./checkpoints/my_checkpoint')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] building autoencoder...\n",
            "(None, 250, 250, 64)\n",
            "64\n",
            "32\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 250, 250, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 250, 250, 32)      896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 250, 250, 32)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 250, 250, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 250, 250, 64)      18496     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 250, 250, 64)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 250, 250, 64)      256       \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4000000)           0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                64000016  \n",
            "=================================================================\n",
            "Total params: 64,019,792\n",
            "Trainable params: 64,019,600\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n",
            "=====================\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 16)]              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4000000)           68000000  \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 250, 250, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTr (None, 250, 250, 64)      36928     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 250, 250, 64)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 250, 250, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTr (None, 250, 250, 32)      18464     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 250, 250, 32)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 250, 250, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTr (None, 250, 250, 3)       867       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 250, 250, 3)       0         \n",
            "=================================================================\n",
            "Total params: 68,056,643\n",
            "Trainable params: 68,056,451\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n",
            "==========================\n",
            "Model: \"autoencoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 250, 250, 3)]     0         \n",
            "_________________________________________________________________\n",
            "encoder (Functional)         (None, 16)                64019792  \n",
            "_________________________________________________________________\n",
            "decoder (Functional)         (None, 250, 250, 3)       68056643  \n",
            "=================================================================\n",
            "Total params: 132,076,435\n",
            "Trainable params: 132,076,051\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "351/351 [==============================] - 8519s 24s/step - loss: 14073.7979 - val_loss: 14047.9990\n",
            "Epoch 2/10\n",
            "351/351 [==============================] - 565s 2s/step - loss: 14060.2393 - val_loss: 14011.3340\n",
            "Epoch 3/10\n",
            "351/351 [==============================] - 566s 2s/step - loss: 14057.6436 - val_loss: 14004.6523\n",
            "Epoch 4/10\n",
            "351/351 [==============================] - 564s 2s/step - loss: 14056.4561 - val_loss: 14003.7334\n",
            "Epoch 5/10\n",
            "351/351 [==============================] - 566s 2s/step - loss: 14055.7227 - val_loss: 14003.2412\n",
            "Epoch 6/10\n",
            "351/351 [==============================] - 565s 2s/step - loss: 14055.1895 - val_loss: 14002.8916\n",
            "Epoch 7/10\n",
            "351/351 [==============================] - 566s 2s/step - loss: 14054.7607 - val_loss: 14002.5811\n",
            "Epoch 8/10\n",
            "351/351 [==============================] - 565s 2s/step - loss: 14054.3789 - val_loss: 14002.2441\n",
            "Epoch 9/10\n",
            "351/351 [==============================] - 566s 2s/step - loss: 14054.0439 - val_loss: 14001.8389\n",
            "Epoch 10/10\n",
            "351/351 [==============================] - 564s 2s/step - loss: 14053.7451 - val_loss: 14001.2861\n",
            "[INFO] saving image data...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}